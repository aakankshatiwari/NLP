{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Task 1: Transform the Text Data for Sentiment Analysis\n",
        "\n",
        "# a. Preprocess Text Data (Common for both datasets):\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0up5cbjRA8B",
        "outputId": "7b7559f5-1cdd-4585-90fe-4f983bb3d73d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the sentiments\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(tweets_df['sentiment'])\n",
        "y = pd.get_dummies(y).values\n",
        "\n",
        "#Develop and Train the Model:\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, SpatialDropout1D, Dense\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100"
      ],
      "metadata": {
        "id": "fq5ibx3iSEhK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=100))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(4, activation='softmax'))  # Assuming 4 classes for sentiment <--- Changed from 3 to 4\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On75D49BRyWZ",
        "outputId": "da4575dd-52c4-4391-aa51-eb1aa289681b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X, y, epochs=5, batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppNTOePpZmt4",
        "outputId": "7c00b6cf-0b5f-4e5b-8fc2-ba826c126e5c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m934/934\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 252ms/step - accuracy: 0.5204 - loss: 1.0958 - val_accuracy: 0.4726 - val_loss: 1.4628\n",
            "Epoch 2/5\n",
            "\u001b[1m934/934\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 248ms/step - accuracy: 0.8540 - loss: 0.3944 - val_accuracy: 0.4592 - val_loss: 1.8038\n",
            "Epoch 3/5\n",
            "\u001b[1m934/934\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 249ms/step - accuracy: 0.9018 - loss: 0.2593 - val_accuracy: 0.4575 - val_loss: 2.2387\n",
            "Epoch 4/5\n",
            "\u001b[1m934/934\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 254ms/step - accuracy: 0.9163 - loss: 0.2115 - val_accuracy: 0.4442 - val_loss: 2.5146\n",
            "Epoch 5/5\n",
            "\u001b[1m934/934\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 248ms/step - accuracy: 0.9247 - loss: 0.1860 - val_accuracy: 0.4398 - val_loss: 2.5152\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c4576cdd480>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and pad the news headlines\n",
        "\n",
        "news_sequences = tokenizer.texts_to_sequences(news_df['cleaned_headline'])\n",
        "news_padded = pad_sequences(news_sequences, maxlen=100)"
      ],
      "metadata": {
        "id": "WUpu5WRLawp8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict Sentiments:\n",
        "\n",
        "# Predict the sentiment for each news headline\n",
        "predicted_sentiments = model.predict(news_padded)\n",
        "\n",
        "# Convert predictions to sentiment labels\n",
        "predicted_sentiment_labels = label_encoder.inverse_transform(predicted_sentiments.argmax(axis=1))\n",
        "\n",
        "# Add predicted sentiment labels to the news dataframe\n",
        "news_df['Predicted Sentiment'] = predicted_sentiment_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKH-FS32Rs9v",
        "outputId": "79b4e1bf-457c-469b-e99a-75ba7211df46"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m32768/32768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m948s\u001b[0m 29ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 3: Evaluate Sentiment Analysis Models Based on Accuracy\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X, y, verbose=1)\n",
        "print(f'Accuracy on Twitter data: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXmHnbMhRnhB",
        "outputId": "5351b4c7-9c20-41d8-c37a-9f007a009d11"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2334/2334\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 30ms/step - accuracy: 0.9322 - loss: 0.1916\n",
            "Accuracy on Twitter data: 0.8431215286254883\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}